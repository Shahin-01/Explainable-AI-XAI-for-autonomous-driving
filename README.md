# Explainable AI-based studies for autonomous driving
In this repository, we provide a compendium of state-of-the-art studies focused on explainable AI (XAI) approaches for autonomous driving. These studies are primarily concerned with developing explainable convolutional neural network (CNN), reinforcement learning (RL), and recurrent neural  networks (RNN)-based AI architectures for autonomous driving. 

1. [VisualBackProp: visualizing CNNs for autonomous driving](https://arxiv.org/abs/1611.05418v1), 2016 </br>
**Summary**: This study explains predictions of CNNs in end-to-end autonomous driving by showing which sets of pixels of an input image contribute to the CNNs' predictions. 

2. [Interpretable learning for self-driving cars by visualizing causal attention](https://openaccess.thecvf.com/content_iccv_2017/html/Kim_Interpretable_Learning_for_ICCV_2017_paper.html), 2017 </br>
**Summary**: The authors use a visual attention model to train a CNN from images to steering angle, and apply a causal filtering to find out which parts of input mainly influence the network's output. 

3. [Textual Explanations for Self-Driving Vehicles](https://openaccess.thecvf.com/content_ECCV_2018/html/Jinkyu_Kim_Textual_Explanations_for_ECCV_2018_paper.html), 2018 </br>
**Summary**: The study uses a visual attention model to train a CNN end-to-end from images and then employs an attention-based video-to-text model to generate textual explanations of the decisive actions. 

4. [Out of Sight But Not Out of Mind: An Answer Set Programming Based Online Abduction Framework for Visual Sensemaking in Autonomous Driving](https://www.ijcai.org/proceedings/2019/260), 2019 </br>
**Summary**: The authors develop an answer set programming-based abductive reasoning framework for online sensemaking that combines knowledge representation and computer vision in an online manner to explain dynamics of traffic scenarios, particularly occlusion scenes.

5. [Visual Scene Understanding for Autonomous Driving Using Semantic Segmentation](https://link.springer.com/chapter/10.1007/978-3-030-28954-6_15), 2019 </br>
**Summary**: The paper propose a semantic segmentation model implemented as a pixel-wise classification that explains underlying real-time perception of the environment.

6. [End-To-End Interpretable Neural Motion Planner](https://openaccess.thecvf.com/content_CVPR_2019/html/Zeng_End-To-End_Interpretable_Neural_Motion_Planner_CVPR_2019_paper.html), 2019 </br>
**Summary**: The authors introduce a neural motion planner for autonomous driving in complex urban scenarios using raw LIDAR data and a HD map.

7. [ I Drive - You Trust: Explaining Driving Behavior Of Autonomous Cars](https://dl.acm.org/doi/abs/10.1145/3290607.3312817?casa_token=2L1pKflGKisAAAAA:_K-4szLXrBBbdUFByQ4r3vJvABTHFdnmZ2QGTy49NhWIFJtVzUQuedpvXPIdviNLIP9x6BuUxwLl), 2019 </br>
**Summary**: A user study is performed: The authors identify a mental model of users for determining an effective practical implementation of an explanation interface for self-driving vehicles.
